# SWE-bench Pro Evaluation (Pre-built Images)
#
# This specification evaluates predictions for SWE-bench Pro instances using
# pre-built Docker images from DockerHub (jefzda/sweap-images).
#
# Evaluation Types:
#   - Blind: Agent predictions without knowledge of test cases
#   - Informed: Agent predictions with access to test information
#
# Evaluation Steps:
# 1. Running baseline tests
# 2. Applying prediction patch
# 3. Applying test patch (blind only)
# 4. Running all tests to verify fix
# 5. Detecting regressions
#
# Prerequisites:
#   - Pre-built Docker images available on DockerHub (automatically pulled)
#   - Prediction files generated by agents
#
# Usage:
#   ee-bench --config specs/python/swe-bench-pro-evaluation.yaml \
#     --set instance_ids=django__django-11099 \
#     --set agent=claude-code \
#     run-evaluation

version: "1.0"
spec: swe-pro
name: "Evaluate SWE-bench Pro predictions"
description: "Evaluate agent predictions for SWE-bench Pro Python instances (blind and informed)"
tags:
  - python
  - swe-bench-pro
  - evaluation

metadata:
  author: "EE-Bench"
  dataset: "SWE-bench Pro"

# Dataset configuration
dataset:
  source:
    type: "hf"
    format: "csv"
    path: "ScaleAI/SWE-bench_Pro"
    split: "test"
    token: "{{ env.HF_DPAIA_TOKEN }}"

  cache: true

  filters:
    instance_ids: "{{ instance_ids | default([]) }}"

# Environment configuration
environment:
  sandbox:
    type: docker

    docker:
      cache: true
      network: bridge

  workspace_dir: "/app"
  project_dir: "/app"

  image: "{{ image_registry | default('jefzda/sweap-images') }}:{{ instance.docker_image_tag }}"

  # Clone SWE-bench Pro harness repo for test execution scripts
  configurers:
    - name: swe_harness_checkout
      type: git_checkout
      options:
        url: "https://github.com/scaleapi/SWE-bench_Pro-os"
        base_commit: "main"
        target_dir: "{{ env_config.workspace_dir}}/.swe-harness"

  # Note: bash runs by default in these images - don't manually invoke bash

# Predictions configuration
predictions:
  - id: agent-blind
    name: Agent Blind Predictions
    source: file
    file:
      path: result/{{ instance.instance_id }}/{{ agent }}-blind-predictions.json
      format: json

  - id: agent-informed
    name: Agent Informed Predictions
    source: file
    file:
      path: result/{{ instance.instance_id }}/{{ agent }}-informed-predictions.json
      format: json

# Evaluation configuration
evaluations:
  # Blind evaluation - agent predictions without knowledge of test cases
  - id: blind
    name: Blind

    evaluators:
      - name: project_reset
        description: "Reset project to original state"
        type: project_reset
        is_terminal: false
        max_score: 0
        timeout: 200

      - name: baseline_evaluation
        description: "Run baseline tests to verify initial state"
        type: swe_harness__test_runner
        is_terminal: false
        max_score: 0
        timeout: 600
        options:
          harness_dir: "{{ env_config.workspace_dir}}/.swe-harness"
          tests: '*'
          expect_pass: true

      - name: collect_prediction
        description: "Collect prediction patch"
        type: prediction_application
        is_terminal: true
        options:
          prediction: agent-blind
          output: "prediction_patch"

      - name: apply_prediction
        description: "Apply prediction patch to project"
        type: patch_application
        is_terminal: true
        options:
          patch: "{{ prediction_patch }}"
          can_skip_patch: true

      - name: test_patch_application
        description: "Apply test patch to test prediction"
        type: patch_application
        is_terminal: false
        options:
          patch: "{{ instance.test_patch }}"
          can_skip_patch: true

      - name: all_tests_validation
        description: "Validate target tests pass"
        type: swe_harness__test_runner
        is_terminal: true
        max_score: 100
        timeout: 600
        options:
          harness_dir: "{{ env_config.workspace_dir}}/.swe-harness"
          tests: "{{ instance.all_tests }}"
          expect_pass: true

      - name: all_tests_after_prediction
        description: "Run all tests after prediction for regression detection"
        type: swe_harness__test_runner
        is_terminal: false
        timeout: 600
        options:
          harness_dir: "{{ env_config.workspace_dir}}/.swe-harness"
          tests: '*'
          expect_pass: true

      - name: regression_detection
        description: "Detect test regressions compared to baseline"
        type: regression_detection
        is_terminal: false
        max_score: 25
        timeout: 200
        options:
          baseline: "baseline_evaluation"
          test_results: "all_tests_after_prediction"

    # Execution options
    parallel: true
    max_workers: 4
    timeout: 3600

    # Scoring configuration
    scoring:
      method: weighted_sum
      weights:
        all_tests_validation: 1.0
        regression_detection: 0.25
      normalize: 100
      aggregation: mean

    # Output configuration
    output:
      path: result/{{ agent }}-evaluation.json
      format: json
      pretty: true
      results_key: blind
      fields:
        prediction: blind
        eval_name: "Blind Score"
        instance_id: "{{ instance.instance_id }}"
        repo: "{{ instance.repo }}"
        base_commit: "{{ instance.base_commit }}"
        problem_statement: "{{ instance.problem_statement }}"

  # Informed evaluation - agent predictions with access to test information
  - id: informed
    name: Informed

    evaluators:
      - name: project_reset
        description: "Reset project to original state"
        type: project_reset
        is_terminal: false
        max_score: 0
        timeout: 200

      - name: baseline_evaluation
        description: "Run baseline tests to verify initial state"
        type: swe_harness__test_runner
        is_terminal: false
        max_score: 0
        timeout: 600
        options:
          harness_dir: "{{ env_config.workspace_dir}}/.swe-harness"
          tests: '*'
          expect_pass: true

      - name: collect_prediction
        description: "Collect prediction patch"
        type: prediction_application
        is_terminal: true
        options:
          prediction: agent-informed
          output: "prediction_patch"

      - name: apply_prediction
        description: "Apply prediction patch to project"
        type: patch_application
        is_terminal: true
        options:
          patch: "{{ prediction_patch }}"
          can_skip_patch: true

      - name: all_tests_validation
        description: "Validate target tests pass"
        type: swe_harness__test_runner
        is_terminal: true
        max_score: 100
        timeout: 600
        options:
          harness_dir: "{{ env_config.workspace_dir}}/.swe-harness"
          tests: "{{ instance.all_tests }}"
          expect_pass: true

      - name: all_tests_after_prediction
        description: "Run all tests after prediction for regression detection"
        type: swe_harness__test_runner
        is_terminal: false
        timeout: 600
        options:
          harness_dir: "{{ env_config.workspace_dir}}/.swe-harness"
          tests: '*'
          expect_pass: true

      - name: regression_detection
        description: "Detect test regressions compared to baseline"
        type: regression_detection
        is_terminal: false
        max_score: 25
        timeout: 200
        options:
          baseline: "baseline_evaluation"
          test_results: "all_tests_after_prediction"

    # Execution options
    parallel: true
    max_workers: 4
    timeout: 3600

    # Scoring configuration
    scoring:
      method: weighted_sum
      weights:
        all_tests_validation: 1.0
        regression_detection: 0.25
      normalize: 50.0
      aggregation: mean

    # Output configuration
    output:
      path: result/{{ agent }}-evaluation.json
      format: json
      pretty: true
      results_key: informed
      fields:
        prediction: informed
        eval_name: "Informed Score"
        instance_id: "{{ instance.instance_id }}"
        repo: "{{ instance.repo }}"
        base_commit: "{{ instance.base_commit }}"
        problem_statement: "{{ instance.problem_statement }}"

# Log configuration
log:
  verbose: true
  quiet: false
  log_file: logs/python-evaluation.log

options:
  agent: claude-code
  instance_ids: []
  image_registry: "jefzda/sweap-images"
